%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make sure to set your name, legi number and url to the right git branch.
\newcommand{\hmwkAuthorName}{Amirreza Bahreini} % Your name
\newcommand{\hmwkAuthorLegi}{11-801-495} % Your name
\newcommand{\hmwkGitBranch}{11-801-495/1\_locally\_linear\_embedding} % Your name
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%	Skip this
%----------------------------------------------------------------------------------------

\documentclass{article}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}
   
%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%	Skip this
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Locally Linear Embedding} % Assignment title
\newcommand{\hmwkDueDate}{Monday,\ March\ 6th,\ 2017} % Due date
\newcommand{\hmwkClass}{SLT coding exercise\ \#1} % Course/class
\newcommand{\hmwkClassTime}{Mo 16:15} % Class/lecture time
\newcommand{\hmwkClassInstructor}{} % Teacher/lecturer

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%	Skip this
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\small{\hmwkClass}}\\
\textmd{\textbf{\hmwkTitle}}\\
\small{https://gitlab.vis.ethz.ch/vwegmayr/slt-coding-exercises}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}
%\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
\vspace{3in}
}

\author{
\hmwkAuthorName\\
\hmwkAuthorLegi
}

\date{ } % Insert date here if you want it to appear below your name

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%	Skip this
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	SECTIONS
%	Now you are in the right hood
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}[The Model]

The model section is intended to allow you to recapitulate the essential ingredients used in \hmwkTitle. Write down the \textit{necessary} equations to specify \hmwkTitle\ and and shortly explain the variables that are involved. This section should only introduce the equations, their solution should be outlined in the implementation section.\newline
Hard limit: One page
\vspace{10pt}

\problemAnswer{ % Answer
The idea behind Locally Linear Embedding (LLE) is that we can learn non linear structure in the data by patching together lots of linear structures in smaller regions. It essentially finds a neighbourhood for each data point, finds the weights for linearly approximating the data in that neighbourhood and finally finds the low-dimensional coordinates best reconstructed by those weights. \\ 
Given the data matrix $\textbf{X}_{n \times p}$, a desired dimension $q < p$ and a fix number of neighbours $k$ for each data point, LLE outputs a matrix $\textbf{Y}_{n \times q}$. Following steps show how LLE works:

\begin{enumerate}
\item Find the $k$ nearest neighbour for each data point $x_i$. Of course here the concept of "near" is directly related to the chosen distance metric.
\item Find the weight matrix $\textbf{W}$ which minimises the following error:
$$\epsilon(\textbf{W}) = \Sigma_i |x_i - \Sigma_j W_{ij}x_j |^2$$
where $W_{ij} = 0$ if $x_j$ does not belong to the set of neighbours found in the first step, and for each $i$ $\Sigma_j W_{ij} = 1$
\item Find the coordinates $\textbf{Y}$ which minimises the reconstruction using the obtained weights in the second step:
$$\Theta(\textbf{Y}) = \Sigma_i |y_i - \Sigma_j W_{ij}y_j|^2$$
where $\Sigma_i Y_{ij} = 0$ and $\textbf{Y}^\intercal\textbf{Y} = \textbf{I}$
\end{enumerate}

Essentially we would like to keep the same neighbouring structure in a lower dimensional space.
}


\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
\begin{homeworkProblem}[The Questions]
This is the core section of your report, which contains the tasks for this exercise and your respective solutions. Make sure you present your results in an illustrative way by making use of graphics, plots, tables, etc. so that a reader can understand the results with a single glance. Check that your graphics have enough resolution or are vector graphics. Consider the use of GIFs when appropriate.\newline
Hard limit: Two pages

\begin{homeworkSection}{(a) Get the data}
For this exercise we will work with the MNIST data set. In order to learn more about it and download it, go to http://yann.lecun.com/exdb/mnist/.
\end{homeworkSection}

\begin{homeworkSection}{(b) Locally linear embedding}
Implement the LLE algorithm and apply it to the MNIST data set. Provide descriptive visualizations for 2D \& 3D embedding spaces. Is it possible to see clusters?
\end{homeworkSection}

\begin{homeworkSection}{(c) Cluster structure}
Investigate the cluster structure of the data. Can you observe block structures in the $M$ matrix (use matrix plots)? Also plot the singular values of $M$. Do you notice something?
Can you think of ways to determine the optimal embedding dimension?
\end{homeworkSection}

\begin{homeworkSection}{(d) Nearest Neighbors}
Investigate the influence of the choice of how many nearest neighbors you take into account. Additionally, try different metrics to find the nearest neighbors (we are dealing with images!).
\end{homeworkSection}

\begin{homeworkSection}{(e) Linear manifold interpolation}
Assume you pick some point in the embedding space. How can you map it back to the original (high dimensional) space? Investigate how well this works for points within and outside the manifold (does it depend on the dimensionality of the embedding space?) Try things like linearly interpolating between two embedding vectors and plot the sequence of images along that line. What happens if you do that in the original space?
\end{homeworkSection}

\vspace{10pt}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/l1_neighs10_dim2_20000}
  \caption{2D embedding}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/l1_neighs10_dim3_20000}
  \caption{3D embedding}
\end{subfigure}
\caption{Embeddings of 20000 data points of MINST dataset using 10 neighbours and L1 distance}
\label{fig:embed}
\end{figure}



\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{results/M_l1_neighs10_dim2_20000}
\caption{Embeddings of 20000 data points of MINST dataset using 10 neighbours and L1 distance}
\label{fig:M}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/singulars_l1_neighs10_dim2_20000}
  \caption{Singular values for all 20000 dimensions}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{results/zoom_singulars_l1_neighs10_dim2_20000}
  \caption{Zoomed on only first few dimensions}
\end{subfigure}
\caption{Singular values of the \textbf{M} matrix}
\label{fig:sings}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=1\linewidth]{results/euclidean_neighs1_dim2_3000}
  \caption{$k=1$}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=1\linewidth]{results/euclidean_neighs11_dim2_3000}
  \caption{$k=11$}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=1\linewidth]{results/euclidean_neighs111_dim2_3000}
  \caption{$k=111$}
\end{subfigure}

\caption{Effect of number of neighbours on embeddings. Notice that the first plot is a normal consequence of having less neighbours than actually desired number of dimensions, here 2. The second plot shows a good clustering compared to all other numbers of neighbours. Last figure is representative of almost any $k > 20$. The reasons is discussed in the answer.}
\label{fig:ks}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=1\linewidth]{results/l1_neighs10_dim2_3000}
  \caption{L1 distance}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=1\linewidth]{results/l2_neighs10_dim2_3000}
  \caption{L2 distance}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=1\linewidth]{results/hamming_neighs10_dim2_3000}
  \caption{Hamming distance}
\end{subfigure}

\caption{Effect of distance metric on the clustering.}
\label{fig:metrics}
\end{figure}


\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
\begin{homeworkProblem}[The Implementation]
In the implementation section you give a concise insight to the practical aspects of this coding exercise. It mainly mentions the optimization methods used to solve the model equations. Did you encounter numerical or efficiency problems? If yes, how did you solve them?
Provide the link to your git branch of this coding exercise.\newline
Hard limit: One page

\vspace{10pt}
\problemAnswer{ % Answer

Many built-in functions of scipy, numpy and sklearn have been used in the code. The entire code is hugely inspired by the function "sklearn.manifold.locally\_linear\_embedding". \\
We may divide the implementation in following three parts: 
\begin{enumerate}

\item Finding neighbours: For this part the function sklearn.neighbors.NearestNeighbors is used.
\item Finding weights: The error function for a particular $x$ can be rewritten as 
$$\epsilon = |x_i - \Sigma_j W_{ij}x_j |^2 = |\Sigma_j W_{ij}(x_i - x_j)|^2 = \Sigma_{jk} W_{ij}W_{ik}C_{jk} $$ 
where $C_{jk} = (x_i - x_j)(x_i - x_k)$ is the local covariance matrix. The optimal weights are then given by $W_{ij} = \frac{\Sigma_k C_{jk}^{-1}}{\Sigma_{lm} C_{lm}^{-1}}$. A simple way avoid this inverse operation is to solve the linear system $\Sigma_j C_{jk}W_{ik} = 1$ and then rescale the weights so that they sum to one (This is easily done by the "solve" function in scipy). The only problem than can arise here is when the covariance matrix is singular, and this can be solved by multiplying a very small regularisation value ($reg=1e-3$) to the trace of $\textbf{C}$.
\item Last step is to find coordinates in the new low-dimensional space. In the exercise session we have seen that the second error function can be rewritten as 
$$\Theta(\textbf{Y}) = \Sigma_i |y_i - \Sigma_j W_{ij}y_j|^2 $$ 
$$\Theta(\textbf{Y}) = \textbf{Y}^\intercal \textbf{M}\textbf{Y}$$ 
where $\textbf{M} = (\textbf{I} - \textbf{W})^\intercal( \textbf{I} - \textbf{W}) = \textbf{I} - \textbf{W} - \textbf{W}^\intercal + \textbf{W}^\intercal\textbf{W}$. \\
Then by Rayleitz-Ritz theorem the optimal embedding is found by computing the bottom $d + 1$ eigenvectors of \textbf{M} where we drop the first one and keep the $d$ remaining. This is also simply done by the use of function "eigh" of scipy.
\end{enumerate}

}
\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
\begin{homeworkProblem}[Your Page]
Your page gives you space to include ideas, observations and results which do not fall into the categories provided by us. You can also use it as an appendix to include things which did not have space in the other sections.\newline
No page limit.

\vspace{10pt}

\problemAnswer{ % Answer
MINST dataset is a very large dataset of $60000$ pictures of $28 \times 28$ dimension. As calculations were very time consuming, I had to reduce the size of dataset to at least $20000$. Some other analysis (like finding the most appropriate number of neighbours) has been performed on a dataset of only $3000$ pictures. At every part of this question, it is explicitly mentioned whether the dataset of $20000$ data points or $3000$ point has been used. \\



Figure \ref{fig:embed} shows both 2D and 3D embeddings. In both embeddings we can see a clustering, although not perfect. One strange thing is that the 3D embedding does not use all the space available to make better clusters. \\
Plotting the $\textbf{M}$ matrix is extremely hard as it is of $20000 \times 20000$ dimension which makes it almost impossible to have a look at it. In order to make it easier to see, only a very tiny fraction of matrix is plotted: $\textbf{M}_{1:50,1:50}$. Figure \ref{fig:M} shows this small block of the matrix which can be representative of the entire matrix, as it is very sparse, with large values on the secondary diagonal and mostly 0s everywhere else (but not everywhere). \\
Figure \ref{fig:sings} shows the singular values of the $\textbf{M}$ matrix. If I understood well, the most appropriate number of dimensions to take in this case would be 5, as the singular values are still very small (and thus the objective value of our optimisation stays small) but we get more dimensions to embed the data. \\

Both effects of number of neighbours and distance metrics have been studied. For this part only $3000$ data points are used. First embeddings for $k$ $in$ $[1, 11, 21, ..., 131, 141]$ have been compared. Visually it could be concluded that $k=11$ results in clusters that are better separated from each other. Figure \ref{fig:ks} compares the embeddings for only few of them (Note that in this case the metric distance is Euclidean, for not any particular reason). One interesting result was that for very big $k$s (aprox. $k > 20$) it seems that the clustering doesn't change that much and it actually doesn't improve at all. This result could come from the fact that when the number of neighbours are larger compared to the curvature of the original space, then we don't get any good resolution; in other words, the neighbourhood becomes so large that it can't be learned with out linear system of weights anymore, as then we try to fit a linear model for a non linear structure. In a Next step, embeddings for $k$ $in$ $[8, 9, 10, ..., 19]$ were compared in order to find out whether there is any "optimal" clustering for a specific $k$. Results was that they were mostly all visually similar regarding how well clusters are separated and thus they are not plotted here any more. That's why for other analysis $k=10$ is chosen. \\
In order to see the effect of distance metric, 2D embeddings for distances L1, L2 and Hamming (Intentionally an almost irrelevant distance metric) are compared. Figure \ref{fig:metrics} shows the result. The result of L1 distance and L2 distance are almost the same (one rotated compared to other, which is actually inherent in solving the \textbf{M} matrix) and this is simply because both distances have the same nature. We can also see the clusterings resulted by the hamming distance are worse as there are more overlapping points from different clusters. This is also an expected result as the hamming distance has no meaning in the context of image vectors.

 

}
\end{homeworkProblem}
\clearpage

\end{document}

